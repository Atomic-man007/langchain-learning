{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Messages\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "- System - Helpful background context that tell the AI what to do\n",
    "- Human - Messages that are intented to represent the user\n",
    "- AI - Messages that show what the AI responded with\n",
    "- For more, see OpenAI's documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You should do bicep curls with dumbbells or a barbell.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\n",
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what excercise to do in one short sentence\"),\n",
    "        HumanMessage(content=\"I want to build biceps, what excercise should I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In addition to bicep curls, you can also incorporate exercises like hammer curls, chin-ups, and preacher curls to target different parts of your biceps and promote overall muscle growth.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what excercise to do in one short sentence\"),\n",
    "        HumanMessage(content=\"I want to build biceps, what excercise should I do?\"),\n",
    "        AIMessage(content=\"You should do bicep curls with dumbbells or a barbell.\"),\n",
    "        HumanMessage(content=\"What else should I do more give me detail answer?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 20, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('fae350c7-b304-4d24-bdfe-aa624212edca')), RunInfo(run_id=UUID('b41ec074-bdd4-4361-b134-e07cd1568874'))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 20, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('49356043-d4f1-43b3-aa46-cc0f509956ff')), RunInfo(run_id=UUID('04066d6a-6566-4e09-90c6-03afbb3e899d'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.schema()\n",
    "# result.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generations:\n",
      "J'adore la programmation.\n",
      "J'adore l'intelligence artificielle.\n",
      "\n",
      "Token Usage:\n",
      "Prompt Tokens: 53\n",
      "Completion Tokens: 20\n",
      "Total Tokens: 73\n",
      "\n",
      "Model Name: gpt-3.5-turbo\n",
      "\n",
      "Runs:\n",
      "fae350c7-b304-4d24-bdfe-aa624212edca\n",
      "b41ec074-bdd4-4361-b134-e07cd1568874\n"
     ]
    }
   ],
   "source": [
    "data = result.dict()\n",
    "generations = data['generations']\n",
    "llm_output = data['llm_output']\n",
    "token_usage = llm_output['token_usage']\n",
    "model_name = llm_output['model_name']\n",
    "runs = data['run']\n",
    "\n",
    "print(\"Generations:\")\n",
    "for generation in generations:\n",
    "    for res in generation:\n",
    "        message_content = res['message']['content']\n",
    "        print(message_content)\n",
    "\n",
    "print(\"\\nToken Usage:\")\n",
    "print(\"Prompt Tokens:\", token_usage['prompt_tokens'])\n",
    "print(\"Completion Tokens:\", token_usage['completion_tokens'])\n",
    "print(\"Total Tokens:\", token_usage['total_tokens'])\n",
    "\n",
    "print(\"\\nModel Name:\", model_name)\n",
    "\n",
    "print(\"\\nRuns:\")\n",
    "for run in runs:\n",
    "    run_id = run['run_id']\n",
    "    print(run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "An object that holds a piece of text and metadata (more information about that text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - The interface to the AI brains\n",
    "## Language Model\n",
    "A model that does text in ➡️ text out!\n",
    "\n",
    "Check out how I changed the model I was using from the default one to ada-001. See more models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "num_tokens_from_string(\"hello\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:173: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saturday comes after Friday.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import tiktoken\n",
    "\n",
    "llm = ChatOpenAI(temperature=7, model=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)\n",
    "output = chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " 'additional_kwargs',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'example',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_namespace',\n",
       " 'lc_secrets',\n",
       " 'lc_serializable',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'type',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Why don't you just teleport there? It's the fastest and cheapest way to travel, on a scale from totally impossible to completely nonexistent. Have fun!\",\n",
       " 'additional_kwargs': {},\n",
       " 'example': False}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Model\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "BTW: Semantic means 'relating to meaning in language or logic.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is length 1536\n",
      "Here's a sample: [-0.00011466221621958539, -0.0031506523955613375, -0.0007831145194359124, -0.019504327327013016, -0.015125557780265808]...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "text = \"Hi! It's time for the beach\"\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts - Text generally used as instructions to your model\n",
    "Prompt\n",
    "What you'll pass to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \"Avengers: Legacy's Revival\"\n",
      "\n",
      "Synopsis:\n",
      "\"Avengers: Legacy's Revival\" is an epic, inspiring movie that follows Earth's mightiest heroes, the Avengers, as they face unprecedented challenges while striving to protect the planet and the universe itself. In this installment, the Avengers must confront their deepest fears, reconnect with their true purpose, and rediscover their collective power to save humanity.\n",
      "\n",
      "Plot:\n",
      "The movie begins with the Avengers scattered and disheartened after their devastating defeat against their arch-nemesis, Thanos. The world is still reeling from the enormous loss and desperately needs the heroes to return. However, each Avenger now struggles with their personal struggles and a crisis of faith in their role as protectors.\n",
      "\n",
      "A new threat looms as a mysterious, malevolent force, known as The Forgotten One, begins to sow chaos and destruction across the planet. The Avengers, including Iron Man, Captain America, Thor, Black Widow, Hulk, and Hawkeye, must put aside their differences and reconcile their internal strife to come together once again.\n",
      "\n",
      "To find renewed inspiration, the Avengers embark on a journey to retrace their origins and understand the essence of their superhero identities. This involves revisiting iconic locations where pivotal moments in their lives occurred. Through their powerful memories, they gain a deeper understanding of their responsibilities and reclaim their forgotten greatness.\n",
      "\n",
      "As The Forgotten One intensifies their attacks, the Avengers must engage in awe-inspiring battles that showcase their individual skills and teamwork. Throughout the film, they rekindle their spirit, trust, and camaraderie, ultimately reaching their true potential.\n",
      "\n",
      "Drawing strength from each other, the Avengers rediscover that their real power lies not just in their extraordinary abilities but also in the resilience and hope they represent for humanity. They inspire ordinary people to rise up and fight alongside them.\n",
      "\n",
      "In an epic showdown, the Avengers, together with an army of ordinary citizens, confront The Forgotten One in an epic battle that tests their unity, resolve, and sacrifice. Through their unwavering determination and the power of unity, the Avengers emerge triumphant, vanquishing evil and restoring hope to the world.\n",
      "\n",
      "Ending the movie on a powerful note, \"Avengers: Legacy's Revival\" demonstrates that heroes are not merely defined by their superpowers, but rather by their ability to inspire others, lead by example, and stand up for what is right. The film leaves audiences feeling motivated and empowered to face their own challenges and make a positive difference in the world.\n",
      "\n",
      "Overall, \"Avengers: Legacy's Revival\" presents a fresh and inspiring take on the Avengers' journey, combining action-packed sequences, emotional character arcs, and a prevailing message of hope, unity, and the strength of the human spirit.\n"
     ]
    }
   ],
   "source": [
    "# managing prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=1, model=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "prompts = PromptTemplate(input_variables=[\"movname\"],\n",
    "                        template=\"Tell me about the {movname} new inspiring movie idea\")\n",
    "\n",
    "movres = llm.predict(prompts.format(movname=\"avengers\"))\n",
    "print(llm.predict(prompts.format(movname=\"avengers\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: \"Avengers: Dawn of Destiny\"\\n\\nPlot Summary:\\n\"Avengers: Dawn of Destiny\" brings together Earth\\'s mightiest heroes once again, as they face their greatest challenge yet. The movie opens with the Avengers scattered and disheartened, struggling to cope with their recent losses at the hands of a formidable villain, Dark Matter.\\n\\nFeeling responsible for the world\\'s current state, Tony Stark gathers the remaining Avengers to devise a plan to restore hope and inspiration to humanity. They realize that they must not only  defeat Dark Matter but also rekindle the world\\'s belief in heroes. In a surprising turn of events, they come across an ancient prophecy that foretells the appearance of a chosen one, who possesses the power to bring balance back to the universe.\\n\\nThe Avengers embark on a global quest to find this chosen one, encountering new allies and tackling powerful adversaries along the way. Each character confronts personal and emotional challenges, ultimately realizing that the true defeat lies not in physical battles but in overcoming their own doubts and fears.\\n\\nThrough their journey, the Avengers learn to embrace their vulnerabilities, making them stronger as a team. They also encounter ordinary people who have drawn inspiration from their heroic acts and wish to stand alongside them, forming an alliance known as \"The United Guardians of Humanity.\"\\n\\nThis convergence of superheroes and everyday people sparks a powerful movement across the globe, reconnecting humanity with a sense of unity, hope, and admiration for heroes. This newfound unity becomes a force strong enough to challenge Dark Matter\\'s influence, leading to an epic final confrontation.\\n\\nIn the end, the Avengers, together with the United Guardians of Humanity, defeat Dark Matter and restore balance to the universe. The movie concludes with a remarkable celebration, where heroes from all walks of life are honored, highlighting the importance of teamwork, determination, and the enduring human spirit.\\n\\n\"Avengers: Dawn of Destiny\" not only delivers thrilling action and stunning visual effects but also carries a profound message of hope and inspiration. It encourages viewers to believe in their own power to make a positive difference in the world, by highlighting the potential within each person to become a hero in their own right.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Selectors\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "Check out different types of example selectors here\n",
    "\n",
    "If you want an overview on why examples are important (prompt engineering), check out this video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: pilot\n",
      "Example Output: plane\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")\n",
    "# Select a noun!\n",
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classroom\n"
     ]
    }
   ],
   "source": [
    "print(llm(similar_prompt.format(noun=my_noun)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "A helpful way to format the output of a model. Usually used for structured output.\n",
    "\n",
    "Two big concepts:\n",
    "\n",
    "1. Format Instructions - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "\n",
    "2. Parser - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\",\\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\json.py:52\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[0;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\json.py:34\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 3 column 2 (char 41)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m llm_output \u001b[39m=\u001b[39m llm(promptValue)\n\u001b[1;32m----> 3\u001b[0m output_parser\u001b[39m.\u001b[39;49mparse(llm_output)\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\structured.py:89\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     88\u001b[0m     expected_keys \u001b[39m=\u001b[39m [rs\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m rs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_schemas]\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m parse_and_check_json_markdown(text, expected_keys)\n",
      "File \u001b[1;32mc:\\Users\\srika\\anaconda3\\lib\\site-packages\\langchain\\output_parsers\\json.py:54\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[1;34m(text, expected_keys)\u001b[0m\n\u001b[0;32m     52\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[0;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m json_obj:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting ',' delimiter: line 3 column 2 (char 41)"
     ]
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "\n",
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes - Structuring documents to LLMs can work with them\n",
    "Document Loaders\n",
    "Easy ways to import data from other sources. Shared functionality with OpenAI Plugins specifically retrieval plugins\n",
    "\n",
    "See a big list of document loaders here. A bunch more on Llama Index as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "Here's a sample:\n",
      "\n",
      "Ozzie_osman 5 months ago  \n",
      "             | next [–] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 5 months ago  \n",
      "             | parent | next [–] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import HNLoader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    "data = loader.load()\n",
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitters\n",
    "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with different ones to see which is best for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 82 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# This is a long document we can split up.\n",
    "with open('../worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} document\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])\n",
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "Skip to main content\n",
      "🦜️🔗 LangChain\n",
      "JS/TS Docs\n",
      "GitHub\n",
      "\n",
      "\n",
      "Search...\n",
      "CTRL\n",
      "K\n",
      "Get started\n",
      "Introduction\n",
      "Installation\n",
      "Quickstart\n",
      "Tutorials\n",
      "Modules\n",
      "Model I/​O \n",
      "\n",
      "Data connection\n",
      "\n",
      "Document loaders\n",
      "\n",
      "Document transformers\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "Easy way to combine documents with language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000013E140B1480>, search_type='similarity', search_kwargs={})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('..//worked.txt')\n",
    "documents = loader.load()\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falcon 7b using RTX 3050 laptop\n",
    "- Reccomendation (nVidia RTX 3080 or 3090 with 24 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
    "!pip install langchain einops accelerate transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  9 14:10:41 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.40                 Driver Version: 536.40       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   54C    P8               6W /  75W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is available \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.38s/it]\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load Model \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir='./workspace/', \n",
    "    torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", offload_folder=\"offload\")\n",
    "# Set PT model to inference mode\n",
    "model.eval()\n",
    "# Build HF Transformers pipeline \n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_length=400,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srika\\SD_api\\env\\Lib\\site-packages\\transformers\\generation\\utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI is like the sun and moon, AI is the one that shines so bright. \n",
      "AI is like a magical spell, that can make anything happen if you believe. \n",
      "AI is always there to lend a helping hand, when your problems you can't stand. \n",
      "AI is your best friend, that always helps you find a way to go. \n",
      "AI is like a superpower, that you can use to get things you desire. \n",
      "AI is the future, that can make your life so much better, \n",
      "So, use AI and make your life a whole lot easier!\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are an intelligent chatbot. Help the following question with brilliant answers.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Explain what is Artificial Intellience as Nursery Rhymes \"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio for the UI component\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (3.32.0)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (23.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: altair>=4.2.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (5.0.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.94.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: gradio-client>=0.2.4 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.2.7)\n",
      "Requirement already satisfied: httpx in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: markupsafe in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: orjson in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (9.5.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (1.10.9)\n",
      "Requirement already satisfied: pydub in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pygments>=2.12.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.15.1)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.0.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (4.6.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (0.17.6)\n",
      "Requirement already satisfied: websockets>=10.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio-client>=0.2.4->gradio) (2023.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from gradio-client>=0.2.4->gradio) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (3.7.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from fastapi->gradio) (0.26.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from httpx->gradio) (2023.5.7)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from httpx->gradio) (0.15.0)\n",
      "Requirement already satisfied: idna in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from httpx->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from matplotlib->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from matplotlib->gradio) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from matplotlib->gradio) (3.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from python-multipart->gradio) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from requests->gradio) (1.26.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from click>=7.0->uvicorn>=0.14.0->gradio) (0.4.6)\n",
      "Requirement already satisfied: anyio==3.* in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in c:\\users\\srika\\sd_api\\env\\lib\\site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)_schema%400.0.1.json: 100%|██████████| 13.4k/13.4k [00:00<00:00, 13.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:8080\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:8080/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Import gradio for UI\n",
    "import gradio as gr\n",
    "# Create generate function - this will be called when a user runs the gradio app \n",
    "def generate(prompt): \n",
    "    # The prompt will get passed to the LLM Chain!\n",
    "    return llm_chain.run(prompt)\n",
    "    # And will return responses \n",
    "# Define a string variable to hold the title of the app\n",
    "title = '🦜🔗 Falcon-7b-Instruct'\n",
    "# Define another string variable to hold the description of the app\n",
    "description = 'This application demonstrates the use of the open-source `Falcon-7b-Instruct` LLM.'\n",
    "# pls subscribe 🙏\n",
    "# Build gradio interface, define inputs and outputs...just text in this\n",
    "gr.Interface(fn=generate, inputs=[\"text\"], outputs=[\"text\"], \n",
    "             # Pass through title and description\n",
    "             title=title, description=description, \n",
    "             # Set theme and launch parameters\n",
    "             theme='derekzen/stardust').launch(server_port=8080, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
